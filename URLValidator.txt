First just do manual testing. Call the valid method of URLValidator with different possible valid/invalid 
inputs and see if you find a failure. (2 points)





Second, come up with good input partitioning. Try to provide a varying set of inputs that partition the 
overall input set well. Did you find any failures? You can call valid method once or more for each partition. (3 points)

-- Input Partitioning Complete




Third, do programming based testing. Write few unit test cases. You can have some sort of loop in your 
unit test and test different url with each instance of loop. Something very similar to testIsValid() but 
your own logic and idea. Even a single test will be sufficient if you write it like testIsValid() method. 
Did you find any failures? Submit your test files and test cases as part of your 
work under your onid/URLValidator folder.   (5 points)

-- Programming based testing Complete




When you find out any failure, debug using Eclipse debugger and try to localize its cause.    
Provide at what line/lines in what file the failure manifested itself. Did you use any of Aganâ€™s 
principle in debugging URLValidator?  (5 points)





Provide a report called URLValidator.pdf/URLValidator.txt (5 points). You need to provide following 
details in the report:





Clearly mention your methodology of testing. 
For manual testing, provide some of your (not all) urls. 




For partitioning, mention your partitions with reasons. 

testDomainPartitions, domain partition test function
The testDomainPartitions method steps through an array of input domains, calling the isValid method on each potential url 
combination. What make this method helpful for testing is each url is passes to the isValid method consists of valid
url piece combinations except for a single element. For example a possible url it will pass to isValid is "http//www.google.com/test/testing?t=1"
what makes this url example helpful is only the url scheme portion of the url is invalid. This lets implements Agan's
5th testing principle "Only change 1 thing at a time" automatically across multiple tests, helping us localize bugs.

Partions
We made 4 different partitions based on how the isValid method checks a url. We divided up each partition into two
separate partitions; one with valid and another with invalid values. This allowed us to toggle one partition at a time
as invalid, helping us to localize faults.

Scheme partitions
Reading through the documentation for valid/invalid schemes it seemed three different things could result in
and scheme being declared as invalid; scheme misspelling, incorrect colon's and brackets, and last not including a scheme.
To support this we divided this partition into 3; first we made the scheme invalid by misspelling http. 
Next we removed the color from http which should should also cause the url to fail. Last we didn't include the
scheme at all.

Authority partitions
Reading through documentation of authority we found how its determined as valid invalid was very similar to
the scheme. It validates IP, urls without www, and urls with www so we broke this partition into 3.
The first invalid value selected was blank, as they are never used. The next was an IP address to large (it had
a 265 in it) and last was an invalid IP address with not enough numbers. Curiously this input partition revealed a bug
in the isValid method with its "valid" partitions. The "www.google.ru" wasn't being passed as valid. A bug report was
written noting this. 

Path partitions
For our path partitions we determined blank paths (paths only with a space character) and paths not beginning
with a "/" are considered invalid. So we divided this partition in 2. The first invalid value is " " and the second
is begins with a letter instead of a "/".

Query partitions
The Query partition was dividing into two. The first invalid test case had invalid
characters which should a cause a failure as noted in the url query standards. The second
was designed to test a query in an invalid format (not following the ?field=value) 
format defined in the query specifications, the test cased didn't include the "?" mark
which indicates the string is a query.



For unit tests/random tests, submit your unit tests using svn under URLValidator folder. 





In report files mention the name of your tests. 





Also mention, how did you work in the team? How did you divide your work? How did you 
collaborate? 

How We Worked Together

We communicated through email and Google Hangouts. To collaborate during test development we relied
heavily on our groups Github repo we set up specifically for this project. At each stage 
of the project, we held a meeting on Google Hangouts and divided the work. For part A, we all contributed to 
the text document. For part B, we each took a different testing method, researched, then developed the method
and pushed our tests and with a writeup to the shared repo. Then we all had another group meeting where each of 
us introduced our test method and walked the rest of the group through the test method. This line-by-line
code review helped ensure our test methods were functioning the way we intended. Once we accepted the test methods
we began debugging. Each of us were assigned a bug we didn't actually locate, that way we had to work through
parts of the code we weren't as familiar with to localize the fault. This helped us better apply Agan's principles
and write non-biased bug reports.




Write bug report for each of the bugs you found. Bug report should follow all the standard bug 
reporting principle, Use apache commons validator issue tracking system issues as standard to write bug report. 






Please mention in the bug report, file name and line number where bug manifested itself. Provide your
 debugging details for each bugs. You don't need to report bugs using svn, just dump your bug report in the 
 URLValidator.pdf/.txt file. 


            
            
            
            
            
            
            
            
            
            
            
            
            